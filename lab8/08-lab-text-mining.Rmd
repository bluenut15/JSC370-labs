---
title: "Lab 08 - Text Mining/NLP"
output: html_document
---

```{r setup}
knitr::opts_chunk$set(eval = T, include  = T)
```

# Learning goals

- Use `unnest_tokens()` and `unnest_ngrams()` to extract tokens and ngrams from text
- Use dplyr and ggplot2 to analyze and visualize text data
- Try a theme model using `topicmodels`

# Lab description

For this lab we will be working with the medical record transcriptions from https://www.mtsamples.com/ available at https://github.com/JSC370/JSC370-2025/tree/main/data/medical_transcriptions.

# Deliverables

1. Questions 1-7 answered, knit to pdf or html output uploaded to Quercus.

2. Render the Rmarkdown document using `github_document` and add it to your github site. Add link to github site in your html.

Link to github repo: https://github.com/bluenut15/JSC370-labs.git


### Setup packages

You should load in `tidyverse`, (or `data.table`), `tidytext`, `wordcloud2`, `tm`, and `topicmodels`.


## Read in the Medical Transcriptions

Loading in reference transcription samples from https://www.mtsamples.com/

```{r}
library(tidytext)
library(tidyverse)
library(wordcloud2)
library(tm)
library(topicmodels)

mt_samples <- read_csv("https://raw.githubusercontent.com/JSC370/JSC370-2025/main/data/medical_transcriptions/mtsamples.csv")
mt_samples <- mt_samples |>
  select(description, medical_specialty, transcription)

head(mt_samples)
```

---

## Question 1: What specialties do we have?

We can use `count()` from `dplyr` to figure out how many different medical specialties are in the data. Are these categories related? overlapping? evenly distributed? Make a bar plot.

```{r}
mt_samples |>
  count(medical_specialty, sort = TRUE) |>
  ggplot(aes(fct_reorder(medical_specialty, n), n)) + 
  geom_col(fill="darkgreen") +
  coord_flip() + 
  theme_bw()
```

---

## Question 2: Tokenize

- Tokenize the the words in the `transcription` column
- Count the number of times each token appears
- Visualize the top 20 most frequent words with a bar plot
- Create a word cloud of the top 20 most frequent words

### Explain what we see from this result. Does it makes sense? What insights (if any) do we get?

```{r}
tokens <- mt_samples |>
  select(transcription) |>
  unnest_tokens(word, transcription) |>
  group_by(word) |>
  summarize(word_frequency = n()) |>
  arrange(across(word_frequency, desc)) |> 
  head(20)

tokens
```

```{r}
tokens |>
  ggplot(aes(fct_reorder(word, word_frequency), word_frequency)) +
  geom_bar(stat = "identity", fill="dodgerblue") +
  coord_flip() + 
  theme_bw()
```

```{r}
tokens |> 
  count(word, sort = TRUE) |> 
  wordcloud2(size = 0.5, color = "random-light", backgroundColor = "black")
```

The top 20 words are mostly stopwords (e.g. the, was, of, etc.). This makes sense, since these words are essential for English grammar. 

---

## Question 3: Stopwords

- Redo Question 2 but remove stopwords
- Check `stopwords()` library and `stop_words` in `tidytext`
- Use regex to remove numbers as well
- Try customizing your stopwords list to include 3-4 additional words that do not appear informative

### What do we see when you remove stopwords and then when you filter further? Does it give us a better idea of what the text is about?

```{r}
head(stopwords("english"))
length(stopwords("english"))
head(stop_words)
```

Remove stopwords and numbers

```{r}
# Additional words to remove
custom_stopwords <- tibble(word = c("mm", "mg", "noted"))

all_stopwords <- bind_rows(stop_words, custom_stopwords)
                           
tokens <- mt_samples |>
  select(transcription) |>
  unnest_tokens(word, transcription) |>
  anti_join(all_stopwords, by="word") |>  # Remove stopwords
  filter(!str_detect(word, "^[0-9]+$")) |>  # Remove numbers
  group_by(word) |>
  summarize(word_frequency = n()) |>
  arrange(across(word_frequency, desc)) |> 
  head(20)
```

Visualizations
```{r}
tokens |>
  ggplot(aes(fct_reorder(word, word_frequency), word_frequency)) +
  geom_bar(stat = "identity", fill="dodgerblue") +
  coord_flip() + 
  theme_bw()

tokens |> 
  count(word, sort = TRUE) |> 
  wordcloud2(size = 0.4, color = "random-light", backgroundColor = "black")

```

After removing stopwords and numbers, the top 20 words appear to be mostly revolving around patients, their medical transcription and health status (e.g. the top 5 words are "patient", "left, "procedure", "history", and "normal"). It suggests that the text is about healthcare.

---



## Question 4: ngrams

Repeat question 2, but this time tokenize into bi-grams. How does the result change if you look at tri-grams? Note we need to remove stopwords a little differently. You don't need to recreate the wordclouds.

```{r}
stop_words2 <- c(stop_words$word, "mm", "mg", "noted")
sw_start <- paste0("^", paste(stop_words2, collapse=" |^"), "$")
sw_end <- paste0("", paste(stop_words2, collapse="$| "), "$")

tokens_bigram <- mt_samples |>
  select(transcription) |>
  unnest_tokens(ngram, transcription, token = "ngrams", n = 2) |>
  filter(!grepl(sw_start, ngram, ignore.case = TRUE)) |> 
  filter(!grepl(sw_end, ngram, ignore.case = TRUE)) |> 
  filter(!grepl("[[:digit:]]+", ngram)) |> 
  group_by(ngram) |>
  summarize(word_frequency = n()) |> 
  arrange(across(word_frequency, desc)) |>
  head(20)
```

Visualizations
```{r}
tokens_bigram |>
  ggplot(aes(ngram, word_frequency)) +
  geom_col(fill="dodgerblue") +
  coord_flip() + 
  theme_bw()

```
---

## Question 5: Examining words

Using the results from the bigram, pick a word and count the words that appear before and after it, and create a plot of the top 20.

```{r}
library(stringr)
# e.g. patient, blood, preoperative...
tokens_bigram_counts <- tokens_bigram |>
  filter(str_detect(ngram, regex("\\sblood$|^blood\\s"))) |> 
  mutate(word = str_remove(ngram, "blood"), word = str_remove_all(word, " ")) |>
  group_by(word) |> 
  head(20)

tokens_bigram_counts |>
  ggplot(aes(reorder(word, word_frequency), word_frequency)) + 
  geom_col(fill="dodgerblue") + 
  theme_bw()

```

---


## Question 6: Words by Specialties

Which words are most used in each of the specialties? You can use `group_by()` and `top_n()` from `dplyr` to have the calculations be done within each specialty. Remember to remove stopwords. How about the 5 most used words?


```{r}
top_words <- mt_samples |>
  unnest_tokens(word, transcription) |>
  anti_join(all_stopwords, by="word") |>  # Remove stopwords
  filter(!str_detect(word, "^[0-9]+$")) |>  # Remove numbers
  group_by(medical_specialty) |>
  count(word, sort = TRUE) |> 
  top_n(5, n) |>  # Get the top 5 words per specialty
  arrange(medical_specialty, desc(n))  # Arrange results

# Display the results
top_words
  
```


## Question 7: Topic Models

See if there are any themes in the data by using a topic model (LDA). 

- you first need to create a document term matrix
- then you can try the LDA function in `topicmodels`. Try different k values.
- create a facet plot of the results from the LDA (see code from lecture)


```{r}
transcripts_dtm <- mt_samples |>
  select(transcription) |>
  unnest_tokens(word, transcription) |>
  anti_join(all_stopwords, by="word") |>  # Remove stopwords
  filter(!str_detect(word, "^[0-9]+$")) |>  # Remove numbers
  DocumentTermMatrix()


transcripts_dtm <- as.matrix(transcripts_dtm)   

transcripts_lda <- LDA(transcripts_dtm, k = 5, control = list(seed = 1234))

```

```{r}
transcripts_top_terms <- tidy(transcripts_lda, matrix = "beta") |> 
  filter(!str_detect(term, "^[0-9]+$")) |>
  group_by(topic) |>
  slice_max(beta, n = 10) |>
  ungroup() |> 
  arrange(topic, beta)

transcripts_top_terms |>
  mutate(term = reorder_within(term, beta, topic)) |>
  ggplot(aes(beta, term, fill = factor(topic))) + 
  geom_col(show.legend=FALSE) + 
  facet_wrap(~topic, scales = "free") + 
  scale_y_reordered() + 
  theme_bw()

```




